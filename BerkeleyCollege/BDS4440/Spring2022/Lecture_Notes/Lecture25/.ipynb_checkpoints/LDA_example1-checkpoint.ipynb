{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d6085a",
   "metadata": {},
   "source": [
    "* Example source: https://www.analyticsvidhya.com/blog/2021/06/part-3-topic-modeling-and-latent-dirichlet-allocation-lda-using-gensim-and-sklearn/\n",
    "* https://github.com/sethns/Latent-Dirichlet-Allocation-LDA-/blob/main/Topic%20Modeling%20_%20Extracting%20Topics_%20Using%20Genism.ipynb\n",
    "* https://github.com/sethns/Latent-Dirichlet-Allocation-LDA-/blob/main/Topic%20Modeling%20_%20Extracting%20Topics_%20Using%20Genism.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dafd7c",
   "metadata": {},
   "source": [
    "The Work Flow for executing LDA in Python\n",
    "\n",
    "1. After importing the required libraries, we will compile all the documents into one list to have the corpus.\n",
    "\n",
    "2. We will perform the following text preprocessing steps (can use either spacy or NLTK libraries for preprocessing):\n",
    "\n",
    "    * Convert the text into lowercase\n",
    "    * Split text into words\n",
    "    * Remove the stop loss words\n",
    "    * Remove the Punctuation, any symbols, and special characters\n",
    "    * Normalize the word (I’ll be using Lemmatization for normalization)\n",
    "\n",
    "\n",
    "The next step is to convert the cleaned text into a numerical representation where the process for gensim and sklearn packages differ:\n",
    "\n",
    "3. For sklearn: Use either the Count vectorizer or TF-IDF vectorizer to transform the Document Term Matrix (DTM) into numerical arrays.\n",
    "\n",
    "4. For gensim: Using gensim for Document Term Matrix(DTM), we don’t need to explicitly create the DTM matrix from scratch. The gensim library has an internal mechanism to create the DTM.\n",
    "\n",
    "\n",
    "The only requirement for the gensim package is that we need to pass the cleaned data in the form of tokenized words.\n",
    "\n",
    "5. Next, we pass the vectorized corpus to the LDA model for both the packages gensim and sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "406bb8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text preprocessing\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# import numpy for matrix operation\n",
    "import numpy as np\n",
    "\n",
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "679e1f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I want to watch a movie this weekend.',\n",
       " 'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.',\n",
       " 'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.',\n",
       " 'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!',\n",
       " 'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books helped to learn so much about how our thoughts impact our biology and how we can all rewire our brains.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D1 = 'I want to watch a movie this weekend.'\n",
    "D2 =  'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.'\n",
    "D3 =  'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.'\n",
    "D4 =  'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!'\n",
    "D5 =  'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books helped to learn so much about how our thoughts impact our biology and how we can all rewire our brains.'\n",
    "corpus = [D1, D2, D3, D4, D5]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1aea8053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['want', 'watch', 'movie', 'weekend'],\n",
       " ['went',\n",
       "  'shopping',\n",
       "  'yesterday',\n",
       "  'new',\n",
       "  'zealand',\n",
       "  'world',\n",
       "  'test',\n",
       "  'championship',\n",
       "  'beating',\n",
       "  'india',\n",
       "  'eight',\n",
       "  'wicket',\n",
       "  'southampton'],\n",
       " ['don’t',\n",
       "  'watch',\n",
       "  'cricket',\n",
       "  'netflix',\n",
       "  'amazon',\n",
       "  'prime',\n",
       "  'good',\n",
       "  'movie',\n",
       "  'watch'],\n",
       " ['movie',\n",
       "  'nice',\n",
       "  'way',\n",
       "  'chill',\n",
       "  'however',\n",
       "  'time',\n",
       "  'would',\n",
       "  'like',\n",
       "  'paint',\n",
       "  'read',\n",
       "  'good',\n",
       "  'book',\n",
       "  'it’s',\n",
       "  'long'],\n",
       " ['blueberry',\n",
       "  'milkshake',\n",
       "  'good',\n",
       "  'try',\n",
       "  'reading',\n",
       "  'dr',\n",
       "  'joe',\n",
       "  'dispenza’s',\n",
       "  'book',\n",
       "  'work',\n",
       "  'gamechanger',\n",
       "  'book',\n",
       "  'helped',\n",
       "  'learn',\n",
       "  'much',\n",
       "  'thought',\n",
       "  'impact',\n",
       "  'biology',\n",
       "  'rewire',\n",
       "  'brain']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply Preprocessing on the corpus\n",
    "# stop Loss words\n",
    "stop = set(stopwords.words('english'))\n",
    "# punctuation\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# Lemmatization\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# One function for all the steps:\n",
    "def clean(doc):\n",
    "    # convert text into Lower case + split into words\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    # remove any stop words present\n",
    "    punc_free =''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    # remove punctuations + normalize the text\n",
    "    normalized =\" \".join(lemma.lemmatize (word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "# clean data stored in a new List\n",
    "clean_corpus = [clean(doc).split() for doc in corpus]\n",
    "clean_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1930daa0",
   "metadata": {},
   "source": [
    "# Implementation of LDA using Gensim\n",
    "* After preprocessing the text, we don’t need to explicitly create the document term matrix (DTM). Gensim package has an internal mechanism to create the DTM.\n",
    "\n",
    "3. Creating Document Term Matrix\n",
    "Using gensim for Document Term Matrix(DTM), we don't need to create the DTM matrix from scratch explicitly. The gensim library has internal mechanism to\n",
    "create the DTM.\n",
    "The only requirement for gensis package is we need to pass the cleaned data in the form of tokenized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4e6df2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie\n",
      "want\n",
      "watch\n",
      "weekend\n",
      "beating\n",
      "championship\n",
      "eight\n",
      "india\n",
      "new\n",
      "shopping\n",
      "southampton\n",
      "test\n",
      "went\n",
      "wicket\n",
      "world\n",
      "yesterday\n",
      "zealand\n",
      "amazon\n",
      "cricket\n",
      "don’t\n",
      "good\n",
      "netflix\n",
      "prime\n",
      "book\n",
      "chill\n",
      "however\n",
      "it’s\n",
      "like\n",
      "long\n",
      "nice\n",
      "paint\n",
      "read\n",
      "time\n",
      "way\n",
      "would\n",
      "biology\n",
      "blueberry\n",
      "brain\n",
      "dispenza’s\n",
      "dr\n",
      "gamechanger\n",
      "helped\n",
      "impact\n",
      "joe\n",
      "learn\n",
      "milkshake\n",
      "much\n",
      "reading\n",
      "rewire\n",
      "thought\n",
      "try\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "# Creating the term dictionary of our corpus that is of all the words (Sepcific to Genism syntax perspective),\n",
    "2\n",
    "# where every unique term is assigned an index.\n",
    "dict_ = corpora.Dictionary(clean_corpus)\n",
    "# The dictionary had 52 unqiue words in the cleaned corpus.\n",
    "for i in dict_.values():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1617848",
   "metadata": {},
   "source": [
    "The next step is to convert the corpus (the list of documents) into a document-term Matrix using the dictionary that we had prepared above. (The vectorizer used here is the Bag of Words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3408812b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1)],\n",
       " [(0, 1), (2, 2), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)],\n",
       " [(0, 1),\n",
       "  (20, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1)],\n",
       " [(20, 1),\n",
       "  (23, 2),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1),\n",
       "  (50, 1),\n",
       "  (51, 1)]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting List of documents (corpus) into Document Term Matrix using the dictionary\n",
    "2\n",
    "doc_term_matrix = [dict_.doc2bow(i) for i in clean_corpus]\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e42ad6",
   "metadata": {},
   "source": [
    "This output implies:\n",
    "\n",
    "Document wise we have the index of the word and its frequency.\n",
    "\n",
    "The 0th word is repeated 1 time, then the 1st word repeated 1, and so on …\n",
    "Next, we implement the LDA model by creating the object and passing the required arguments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b36620e",
   "metadata": {},
   "source": [
    "This output implies:\n",
    "\n",
    "Document wise we have the index of the word and its frequency.\n",
    "The 0th word is repeated 1 time, then the 1st word repeated 1, and so on …\n",
    "Next, we implement the LDA model by creating the object and passing the required arguments:\n",
    "    \n",
    "4. Implementation of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af5e2610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.136*\"watch\" + 0.084*\"movie\" + 0.060*\"good\" + 0.060*\"amazon\" + 0.060*\"netflix\" + 0.060*\"prime\" + 0.060*\"cricket\" + 0.060*\"don’t\" + 0.029*\"want\" + 0.027*\"weekend\"'),\n",
       " (1,\n",
       "  '0.074*\"weekend\" + 0.070*\"want\" + 0.065*\"movie\" + 0.063*\"watch\" + 0.015*\"don’t\" + 0.015*\"good\" + 0.015*\"book\" + 0.015*\"cricket\" + 0.015*\"prime\" + 0.015*\"new\"'),\n",
       " (2,\n",
       "  '0.052*\"book\" + 0.028*\"good\" + 0.028*\"blueberry\" + 0.028*\"try\" + 0.028*\"helped\" + 0.028*\"reading\" + 0.028*\"joe\" + 0.028*\"work\" + 0.028*\"dispenza’s\" + 0.028*\"brain\"'),\n",
       " (3,\n",
       "  '0.020*\"championship\" + 0.020*\"wicket\" + 0.020*\"southampton\" + 0.020*\"yesterday\" + 0.020*\"went\" + 0.020*\"india\" + 0.020*\"shopping\" + 0.020*\"new\" + 0.020*\"world\" + 0.020*\"zealand\"'),\n",
       " (4,\n",
       "  '0.051*\"movie\" + 0.051*\"book\" + 0.051*\"paint\" + 0.051*\"long\" + 0.051*\"like\" + 0.051*\"read\" + 0.051*\"time\" + 0.051*\"chill\" + 0.051*\"would\" + 0.051*\"however\"'),\n",
       " (5,\n",
       "  '0.019*\"weekend\" + 0.019*\"watch\" + 0.019*\"movie\" + 0.019*\"good\" + 0.019*\"want\" + 0.019*\"don’t\" + 0.019*\"book\" + 0.019*\"cricket\" + 0.019*\"prime\" + 0.019*\"new\"')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the object for LDA model using gensim Library\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "# Running and Training LDA model on the document term matrix.\n",
    "Ldamodel = Lda(doc_term_matrix, num_topics=6, id2word = dict_, passes=1, random_state=0, eval_every=None)\n",
    "Ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679b61f",
   "metadata": {},
   "source": [
    "This output means: each of the 52 unique words is given weights based on the topics. In other words, it implies which of the words dominate the topics.\n",
    "\n",
    "Now, we find the topics from Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "538953c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.136*\"watch\" + 0.084*\"movie\" + 0.060*\"good\" + 0.060*\"amazon\" + 0.060*\"netflix\"'), (1, '0.074*\"weekend\" + 0.070*\"want\" + 0.065*\"movie\" + 0.063*\"watch\" + 0.015*\"don’t\"'), (2, '0.052*\"book\" + 0.028*\"good\" + 0.028*\"blueberry\" + 0.028*\"try\" + 0.028*\"helped\"'), (3, '0.020*\"championship\" + 0.020*\"wicket\" + 0.020*\"southampton\" + 0.020*\"yesterday\" + 0.020*\"went\"'), (4, '0.051*\"movie\" + 0.051*\"book\" + 0.051*\"paint\" + 0.051*\"long\" + 0.051*\"like\"'), (5, '0.019*\"weekend\" + 0.019*\"watch\" + 0.019*\"movie\" + 0.019*\"good\" + 0.019*\"want\"')]\n"
     ]
    }
   ],
   "source": [
    "print(Ldamodel.print_topics(num_topics=6, num_words=5))\n",
    "# num topics mean: how many topics want to extract\n",
    "# num words: the number of words that want per topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7efd90a",
   "metadata": {},
   "source": [
    "This returns the following six topics (indexed from 0,1,2,3,4,5) with the five words in each of the topics along with their respective weights.\n",
    "\n",
    "Now, we assign these resultant topics to the documents via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d50666f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc : 0 [(0, 0.28205714), (1, 0.5845116), (2, 0.033334), (3, 0.033336144), (4, 0.033424977), (5, 0.033336177)]\n",
      "doc : 1 [(0, 0.011905564), (1, 0.011906144), (2, 0.94046915), (3, 0.011907217), (4, 0.011905454), (5, 0.01190652)]\n",
      "doc : 2 [(0, 0.9166039), (1, 0.016687563), (2, 0.016676312), (3, 0.016667562), (4, 0.016697042), (5, 0.016667575)]\n",
      "doc : 3 [(0, 0.011138678), (1, 0.011119543), (2, 0.011127169), (3, 0.011111964), (4, 0.94439065), (5, 0.011111975)]\n",
      "doc : 4 [(2, 0.9602892)]\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in Ldamodel[doc_term_matrix]:\n",
    "    print(\"doc :\", count, i)\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f8e2d",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "The five documents are assigned the topics with the weightage that will help to tell which is the dominant topic for the respective document.\n",
    "From above can see:\n",
    "\n",
    "1. Document 1 has the highest weight of 58.4% for Topic 2.\n",
    "2. Topic 3 dominates the document 2 having the weightage of 94%. Similarly, 1st topic is the main topic for document 3 with ~92% weight.\n",
    "3. Document 3 is influenced by the Topic 5 with 94% and Topic 4 rules the document 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2903af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
