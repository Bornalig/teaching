{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7 BDS4440\n",
    "\n",
    "# Analyzing a data set: AMES IOWA Real Estate Data\n",
    "\n",
    "Go to the Kaggle site: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
    "and download the AMES Iowa data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: \n",
    "\n",
    "There is one feature that is aboslutely useless in predicting the sale price of the home. Find it and drop it from both the train and test sets. Look at the names of the features and it should be somewhat obvious what that feature is in contrast to the rest of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: \n",
    "\n",
    "There Seperate the categorical features from numericals. Do this in only a few lines of code (less than 10 lines of code).\n",
    "\n",
    "The net out put should be two lists for the feature names:\n",
    "\n",
    "* `categorical=[featureA, featureB,...]`\n",
    "* `numerical=[feactureX, featureY,...]`\n",
    "\n",
    "\n",
    "* Be sure to exclude Sale price because it is not a predicting feature (its the target!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: \n",
    "For each numerical feature, plot the scatter plot of the feature versus the target (SalePrice). Use the techniques from the lectures to plot an ordinary least squares regression line to that feature. Put these plots into  a series of subplots.\n",
    "\n",
    "For the subplots use `plt.subplots(nrows=12, ncols=3, figsize=(20, 80))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4:\n",
    "Look up a beeswarm plot: https://seaborn.pydata.org/generated/seaborn.swarmplot.html\n",
    "\n",
    "Repeat the same process as Problem 3 but this time with a beeswarm plot. You can ignore the part about fitting a regression line for this problem.\n",
    "\n",
    "Use the parameters\n",
    "\n",
    "`nrows=15\n",
    "ncols=3\n",
    "plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 100))`\n",
    "\n",
    "for the subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 \n",
    "\n",
    "For the categorical features, use the chi2 goodness of fit test to see if each feature is useful in predicting the regression value. Loop through all the features, and if the null hypothesis is not rejected, then append it to a list of useless categorical features. Use `alpha=0.01`. Remember to set up your observed values and expected values in a way that allows you to run the test correct;\n",
    "\n",
    "HINT: \n",
    "\n",
    "`useless_categorical_features = []\n",
    "for col in categorical:\n",
    "    observed = list(train[[col,target]].groupby(col).sum().reset_index()[target])\n",
    "    expected = [sum(observed)/len(observed) for i in range(len(observed))]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6: Do the same thing, with the one way ANOVA test\n",
    "\n",
    "But is the `SalePrice` data normal (gaussian)? It may not be. Convert it to normality (for testing purposes) using the `numpy` function `np.log1p(data)` before putting the categorical features into the one way anova test.\n",
    "\n",
    "Hint: \n",
    "\n",
    "some of the code includes\n",
    "\n",
    "`for col in categorical:\n",
    "    data = train[[col,target]].groupby(col)[target].agg(lambda x: list(x))\n",
    "    test = one_way_anova(*data,alpha=0.01)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
