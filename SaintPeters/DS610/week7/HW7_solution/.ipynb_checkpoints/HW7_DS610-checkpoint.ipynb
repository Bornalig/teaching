{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfa304c",
   "metadata": {},
   "source": [
    "# HW7 DS610\n",
    "\n",
    "Streaming Data\n",
    "\n",
    "In this homework you will be running the code below to demonstrate a basic example of perform a streamed query.\n",
    "\n",
    "There are two files attached;\n",
    "* logfile1.json\n",
    "* logfile2.json\n",
    "\n",
    "Put this jupyter notebook into a local EMTPY folder on your computer. Be sure that java is installed on your computer!\n",
    "* If you dont have it, download it from here; https://www.oracle.com/java/technologies/javase-downloads.html\n",
    "* Check your terminal by typing \"java -version\" to see if you have it downloaded\n",
    "\n",
    "Once you have this notebook in an EMPTY folder, then one at a time, add logfile1.json and logfile2.json to the folder AFTER running the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6cbe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: HW7\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/07 19:15:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/07 19:15:06 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/hj/xyjyq1ds11n1ycjbp53_4svw0000gp/T/temporary-a8f204f6-9078-4bb5-86c9-149321b1e59f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/02/07 19:15:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+-----------------+-----------------+\n",
      "|User|max(Arrival_Time)|min(Arrival_Time)|\n",
      "+----+-----------------+-----------------+\n",
      "|null|             null|             null|\n",
      "+----+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----+--------------------+--------------------+\n",
      "|User|   max(Arrival_Time)|   min(Arrival_Time)|\n",
      "+----+--------------------+--------------------+\n",
      "|   g|2015-02-23 05:57:...|2015-02-23 05:18:...|\n",
      "|null|                null|                null|\n",
      "|   f|2015-02-24 07:32:...|2015-02-24 06:56:...|\n",
      "|   e|2015-02-24 09:51:...|2015-02-24 09:13:...|\n",
      "|   h|2015-02-23 09:17:...|2015-02-23 08:45:...|\n",
      "|   d|2015-02-24 08:11:...|2015-02-24 07:41:...|\n",
      "|   c|2015-02-23 07:45:...|2015-02-23 07:12:...|\n",
      "|   i|2015-02-24 06:49:...|2015-02-24 06:12:...|\n",
      "|   b|2015-02-24 09:08:...|2015-02-24 08:30:...|\n",
      "|   a|2015-02-23 08:35:...|2015-02-23 08:03:...|\n",
      "+----+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----+--------------------+--------------------+\n",
      "|User|   max(Arrival_Time)|   min(Arrival_Time)|\n",
      "+----+--------------------+--------------------+\n",
      "|   g|2015-02-23 05:57:...|2015-02-23 05:18:...|\n",
      "|null|                null|                null|\n",
      "|   f|2015-02-24 07:32:...|2015-02-24 06:56:...|\n",
      "|   e|2015-02-24 09:51:...|2015-02-24 09:13:...|\n",
      "|   h|2015-02-23 09:17:...|2015-02-23 08:45:...|\n",
      "|   d|2015-02-24 08:11:...|2015-02-24 07:41:...|\n",
      "|   c|2015-02-23 07:45:...|2015-02-23 07:12:...|\n",
      "|   i|2015-02-24 06:49:...|2015-02-24 06:12:...|\n",
      "|   b|2015-02-24 09:08:...|2015-02-24 08:30:...|\n",
      "|   a|2015-02-23 08:35:...|2015-02-23 08:03:...|\n",
      "+----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "except:\n",
    "    !pip install pyspark\n",
    "    from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType\n",
    "\n",
    "import os\n",
    "current_dir = os.path.abspath(os.curdir)\n",
    "\n",
    "schema = '''\n",
    "StructType([StructField('Arrival_Time', LongType(), True), \n",
    "StructField('Creation_Time', LongType(), True), StructField('Device', StringType(), True),\n",
    "StructField('Index', LongType(), True), StructField('Model', StringType(), True),\n",
    "StructField('User', StringType(), True), StructField('gt', StringType(), True),\n",
    "StructField('x', DoubleType(), True), StructField('y', DoubleType(), True), \n",
    "StructField('z', DoubleType(), True)])\n",
    "'''\n",
    "\n",
    "spark = SparkSession.builder.config(\"HW7\", current_dir).appName(\"StructuredStreaming\").getOrCreate()\n",
    "streaming = spark.readStream.schema(eval(schema)).json(current_dir)\n",
    "# Run forever until terminated\n",
    "\n",
    "stream = streaming.withColumn('Arrival_Time',f.to_timestamp((f.col('Arrival_Time')/1000)))\\\n",
    "                  .withColumn('Creation_Time',f.to_timestamp((f.col('Creation_Time')/1000000000)))\\\n",
    "                  .groupby('User')\\\n",
    "                  .agg(f.max(f.col('Arrival_Time')),f.min(f.col('Arrival_Time')))\n",
    "\n",
    "query = stream.writeStream.outputMode(\"complete\").format(\"console\").queryName(\"arrivals\").start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "# Cleanly shut down the session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4820adf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
